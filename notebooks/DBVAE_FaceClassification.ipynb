{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ab080-0abb-4505-aff5-827d880a670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import h5py\n",
    "import time\n",
    "from IPython import display as ipythondisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# Check that we are using a GPU, if not switch runtimes\n",
    "#   using Runtime > Change Runtime Type > GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "  raise ValueError(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a9b06-8548-4919-ab7b-105583d009b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = Path.cwd() / \".cache\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_to_training_data = CACHE_DIR.joinpath(\"train_face.h5\")\n",
    "\n",
    "if path_to_training_data.is_file():\n",
    "    print(f\"Using cached training data from {path_to_training_data}\")\n",
    "else:\n",
    "    print(f\"Downloading training data to {path_to_training_data}\")\n",
    "    url = \"https://www.dropbox.com/s/hlz8atheyozp1yx/train_face.h5?dl=1\"\n",
    "    torch.hub.download_url_to_file(url, path_to_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15977807-bbc8-45c5-acae-52d403af4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDatasetLoader(Dataset):\n",
    "    def __init__(self, data_path, channels_last=True):\n",
    "        print(f\"Opening {data_path}\")\n",
    "        self.cache = h5py.File(data_path, \"r\")\n",
    "        self.images = self.cache[\"images\"][:]\n",
    "        self.labels = self.cache[\"labels\"][:].astype(np.float32)\n",
    "        self.channels_last = channels_last\n",
    "        self.image_dims = self.images.shape\n",
    "\n",
    "        n_train_samples = self.image_dims[0]\n",
    "        #Array of n_train_samples shuffled randomly\n",
    "        self.train_inds = np.random.permutation(np.arange(n_train_samples))\n",
    "        self.pos_train_inds = self.train_inds[self.labels[self.train_inds, 0] == 1.0]\n",
    "        self.neg_train_inds = self.train_inds[self.labels[self.train_inds, 0] != 1.0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_inds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # normalize to [0,1]\n",
    "        img = (img[:, :, ::-1] / 255.0).astype(np.float32)\n",
    "\n",
    "        if not self.channels_last:  # convert to [H, W, C] to [C,H,W]\n",
    "            img = np.transpose(img, (2,0,1))\n",
    "\n",
    "        return torch.tensor(img), torch.tensor(label)\n",
    "\n",
    "    def get_train_steps_per_epoch(self, batch_size, factor=10):\n",
    "        return self.__len__() // factor // batch_size\n",
    "\n",
    "    def get_batch(self, n, only_faces=False, p_pos=None, p_neg=None, return_inds=False):\n",
    "        if only_faces:\n",
    "            select_inds = np.random.choice(\n",
    "                self.pos_train_inds, size=n, replace=False, p=p_pos)\n",
    "        else:\n",
    "            selected_pos_inds = np.random.choice(\n",
    "                self.pos_train_inds, size=n//2, replace=False, p=p_pos)\n",
    "            selected_neg_inds = np.random.choice(\n",
    "                self.neg_train_inds, size=n//2, replace=False, p=p_neg)\n",
    "            selected_inds = np.concatenate((selected_pos_inds, selected_neg_inds))\n",
    "\n",
    "        sorted_inds = np.sort(selected_inds)\n",
    "        train_img = (self.images[sorted_inds, :,:, ::-1] / 255.0).astype(np.float32)\n",
    "        train_label = self.labels[sorted_inds, ...]\n",
    "\n",
    "        if not self.channels_last:\n",
    "            train_img = np.ascontiguousarray(\n",
    "                np.transpose(train_img, (0,3,1,2)))\n",
    "        return (\n",
    "            (train_img, train_label, sorted_inds)\n",
    "            if return_inds\n",
    "            else (train_img, train_label))\n",
    "\n",
    "    def get_n_most_prob_faces(self, prob, n):\n",
    "        \"\"\"\n",
    "        From the positive training set, sort by probability, look at the top 10n, \n",
    "        take every 10th one to get n images, normalize them, and return them.\n",
    "        \"\"\"\n",
    "        idx = np.argsort(prob)[::-1]\n",
    "        most_prob_inds = self.pos_train_inds[idx[: 10 * n : 10]]\n",
    "        return (self.images[most_prob_inds, ...] / 255.0).astype(np.float32)\n",
    "\n",
    "    def get_all_train_faces(self):\n",
    "        return self.images[self.pos_train_inds]\n",
    "\n",
    "loader = TrainDatasetLoader(path_to_training_data, channels_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae82e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Examining the CelebA training dataset ###\n",
    "\n",
    "# @title Change the sliders to look at positive and negative training examples! { run: \"auto\" }\n",
    "number_of_training_examples = len(loader)\n",
    "(images, labels) = loader.get_batch(100)\n",
    "B, C, H, W = images.shape\n",
    "print(B, C, H, W)\n",
    "face_images = images[np.where(labels == 1)[0]].transpose(0, 2, 3, 1) #(N, C, H, W)  â†’  (N, H, W, C)\n",
    "not_face_images = images[np.where(labels == 0)[0]].transpose(0, 2, 3, 1)\n",
    "\n",
    "face_indices = np.random.choice(len(face_images), 4, replace=False)\n",
    "not_face_indices = np.random.choice(len(not_face_images), 4, replace=False)\n",
    "\n",
    "# Create a 2x4 subplot grid\n",
    "fig, axes = plt.subplots(2, 4, figsize=(8, 8))\n",
    "\n",
    "# Display face images in the first row\n",
    "for i, idx in enumerate(face_indices):\n",
    "    axes[0, i].imshow(face_images[idx])\n",
    "    axes[0, i].set_title(f\"Face #{idx}\")\n",
    "    axes[0, i].grid(False)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Display non-face images in the second row\n",
    "for i, idx in enumerate(not_face_indices):\n",
    "    axes[1, i].imshow(not_face_images[idx])\n",
    "    axes[1, i].set_title(f\"Not Face #{idx}\")\n",
    "    axes[1, i].grid(False)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e692c5b-a6e3-488e-afe6-7f8266ea6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 12\n",
    "in_channels = images.shape[1]\n",
    "def make_standard_classifier(n_outputs):\n",
    "    \"\"\"Standard CNN classifier.\"\"\"\n",
    "\n",
    "    class ConvBlock(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = self.bn(x)\n",
    "            x = self.relu(x)\n",
    "            return x\n",
    "    \n",
    "    model = nn.Sequential(\n",
    "        ConvBlock(in_channels, n_filters, kernel_size=5, stride=2, padding=2),\n",
    "        ConvBlock(n_filters, 2*n_filters, kernel_size=5, stride=2, padding=2),\n",
    "        ConvBlock(2*n_filters, 4*n_filters, kernel_size=3, stride=2, padding=1),\n",
    "        ConvBlock(4*n_filters, 8*n_filters, kernel_size=3, stride=2, padding=1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(H // 16 * W // 16 * 8 * n_filters, 512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(512, n_outputs),\n",
    "        )\n",
    "    return model.to(device)\n",
    "\n",
    "standard_classifier = make_standard_classifier(n_outputs=1)\n",
    "print(standard_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory:\n",
    "    def __init__(self, smoothing_factor=0.0):\n",
    "        self.alpha = smoothing_factor\n",
    "        self.loss = []\n",
    "\n",
    "    def append(self, value):\n",
    "        self.loss.append(\n",
    "            self.alpha * self.loss[-1] + (1 - self.alpha) * value\n",
    "            if len(self.loss) > 0\n",
    "            else value\n",
    "        )\n",
    "\n",
    "    def get(self):\n",
    "        return self.loss\n",
    "\n",
    "class PeriodicPlotter:\n",
    "    def __init__(self, sec, xlabel=\"\", ylabel=\"\", scale=None):\n",
    "        self.xlabel = xlabel\n",
    "        self.ylabel = ylabel\n",
    "        self.sec = sec\n",
    "        self.scale = scale\n",
    "\n",
    "        self.tic = time.time()\n",
    "\n",
    "    def plot(self, data):\n",
    "        if time.time() - self.tic > self.sec:\n",
    "            plt.cla()\n",
    "\n",
    "            if self.scale is None:\n",
    "                plt.plot(data)\n",
    "            elif self.scale == \"semilogx\":\n",
    "                plt.semilogx(data)\n",
    "            elif self.scale == \"semilogy\":\n",
    "                plt.semilogy(data)\n",
    "            elif self.scale == \"loglog\":\n",
    "                plt.loglog(data)\n",
    "            else:\n",
    "                raise ValueError(\"unrecognized parameter scale {}\".format(self.scale))\n",
    "\n",
    "            plt.xlabel(self.xlabel)\n",
    "            plt.ylabel(self.ylabel)\n",
    "            ipythondisplay.clear_output(wait=True)\n",
    "            ipythondisplay.display(plt.gcf())\n",
    "\n",
    "            self.tic = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8281751-d101-462c-be9a-ad9036801e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the standard CNN ###\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "# Training hyperparameters\n",
    "params = dict(\n",
    "    batch_size=32,\n",
    "    num_epochs=2,  # keep small to run faster\n",
    "    learning_rate=5e-4,\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    standard_classifier.parameters(), lr=params[\"learning_rate\"]\n",
    ")  # define our optimizer\n",
    "if hasattr(tqdm, \"_instances\"):\n",
    "    tqdm._instances.clear()  # clear if it exists\n",
    "\n",
    "# set the model to train mode\n",
    "standard_classifier.train()\n",
    "\n",
    "\n",
    "def standard_train_step(x, y):\n",
    "    x = torch.from_numpy(x).float().to(device)\n",
    "    y = torch.from_numpy(y).float().to(device)\n",
    "\n",
    "    # clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # feed the images into the model\n",
    "    logits = standard_classifier(x)\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(logits, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# The training loop!\n",
    "step = 0\n",
    "loss_history = LossHistory(smoothing_factor=0.99)\n",
    "plotter = PeriodicPlotter(sec=2, scale=\"semilogy\")\n",
    "for epoch in range(params[\"num_epochs\"]):\n",
    "    for idx in tqdm(range(len(loader) // params[\"batch_size\"])):\n",
    "        # Grab a batch of training data and propagate through the network\n",
    "        x, y = loader.get_batch(params[\"batch_size\"])\n",
    "        loss = standard_train_step(x, y)\n",
    "        loss_value = loss.detach().cpu().numpy()\n",
    "        loss_history.append(loss_value)\n",
    "        plotter.plot(loss_history.get())\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba22d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_classifier.eval()\n",
    "\n",
    "#Evaluate on a subset of CelebA+Imagenet\n",
    "(batch_x, batch_y) = loader.get_batch(5000)\n",
    "batch_x = torch.from_numpy(batch_x).float().to(device)\n",
    "batch_y = torch.from_numpy(batch_y).float().to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_pred_logits = standard_classifier(batch_x)\n",
    "    y_pred_std = torch.round(torch.sigmoid(y_pred_logits))\n",
    "    acc_std = torch.mean((batch_y == y_pred_std).float())\n",
    "\n",
    "print(\"Standard CNN acuracy training set (biased): {:.4f}\".format(acc_std.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b4f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_faces(channels_last = True):\n",
    "    images = {\"LF\": [], \"LM\": [], \"DF\": [], \"DM\": []}\n",
    "    for key in images.keys():\n",
    "        FILES_PATH = Path.cwd() / \"data\"/ \"faces\"/ key/ \"*.png\"\n",
    "        files = glob.glob(str(FILES_PATH))\n",
    "        for file in sorted(files):\n",
    "            image = cv2.resize(cv2.imread(file), (64,64))[:,:,::-1] / 255.0\n",
    "            if not channels_last:\n",
    "                image = np.transpose(image, (2,0,1))\n",
    "            images[key].append(image)\n",
    "    return images[\"LF\"], images[\"LM\"], images[\"DF\"], images[\"DM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_faces = get_test_faces(channels_last=False)\n",
    "keys = [\"Light Female\", \"Light Male\", \"Dark Female\", \"Dark Male\"]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(keys), figsize=(7.5, 7.5))\n",
    "for i, (group, key) in enumerate(zip(test_faces, keys)):\n",
    "    axs[i].imshow(np.hstack(group).transpose(1,2,0))\n",
    "    axs[i].set_title(key, fontsize=15)\n",
    "    axs[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_classfier_prob_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for x in test_faces:\n",
    "        x = torch.from_numpy(np.array(x, dtype=np.float32)).to(device)\n",
    "        logits = standard_classifier(x)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        probs = torch.squeeze(probs, dim=-1)\n",
    "        std_classfier_prob_list.append(probs.cpu().numpy())\n",
    "\n",
    "std_classfier_probs = np.stack(std_classfier_prob_list, axis=0)\n",
    "\n",
    "x_keys = range(len(keys))\n",
    "y_prob = std_classfier_probs.mean(axis=1)\n",
    "plt.bar(x_keys, y_prob)\n",
    "plt.xticks(x_keys, keys)\n",
    "plt.title(\"Standard Classifier predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12417b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(x, x_recon, mu, logsima, kl_weight=0.0005):\n",
    "    \"\"\"\n",
    "    VAE Loss Function\n",
    "        Computes the loss for the Variational Autoencoder (VAE) model.\n",
    "        The loss is a combination of the reconstruction loss and the KL divergence loss.\n",
    "        Args:\n",
    "            x (torch.Tensor): Original input images.\n",
    "            x_recon (torch.Tensor): Reconstructed images from the VAE.\n",
    "            mu (torch.Tensor): Mean of the latent variable distribution.\n",
    "            logsima (torch.Tensor): Log variance of the latent variable distribution.\n",
    "            kl_weight (float): Weight for the KL divergence loss term.\n",
    "        Returns:\n",
    "            torch.Tensor: Total loss (reconstruction + KL divergence).\n",
    "\n",
    "    \"\"\"\n",
    "    latent_loss = 0.5 * torch.sum(torch.exp(logsima) + mu**2 - 1.0 - logsima)\n",
    "    reconstruction_loss = torch.mean(torch.abs(x - x_recon  ))\n",
    "    return kl_weight * latent_loss + reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361717f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_reparameterization(mu, logsima):\n",
    "    \"\"\"\n",
    "    Reparameterization trick to sample from N(mu, sigma^2) from N(0,1) (Gaussian distribution).\n",
    "    Args:\n",
    "        mu (torch.Tensor): Mean of the latent variable distribution.\n",
    "        logsima (torch.Tensor): Log variance of the latent variable distribution.\n",
    "    Returns:\n",
    "        torch.Tensor: Sampled latent variable.\n",
    "    \"\"\"\n",
    "    eps = torch.randn_like(mu)\n",
    "    sigma = torch.exp(logsima)\n",
    "    return mu + sigma * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e72f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debiasing_loss_function(x, x_pred, y, y_logits, mu, logsigma):\n",
    "    \"\"\"\n",
    "    DV-VAE Loss function\n",
    "        Computes the loss function for the Debiased Variation Autoencoder Model.\n",
    "        The total loss is the mean combination of the classification loss and the VAE loss if the classification true result is a face.\n",
    "        Args:\n",
    "            x (torch.Tensor): Original input images.\n",
    "            x_pred (torch.Tensor): Reconstructed images from the VAE.\n",
    "            y (torch.Tensor): True labels for the input images.\n",
    "            y_logits (torch.Tensor): Predicted logits from the classifier.\n",
    "            mu (torch.Tensor): Mean of the latent variable distribution.\n",
    "            logsigma (torch.Tensor): Log variance of the latent variable distribution.\n",
    "        Returns:\n",
    "            torch.Tensor: Total loss (classification + VAE loss for faces).\n",
    "            torch.tensor: Classification loss.\n",
    "    \"\"\"\n",
    "    vae_loss = vae_loss_function(x, x_pred, mu, logsigma, 0.0005)\n",
    "    classification_loss = F.binary_cross_entropy_with_logits(y_logits, y, reduction='none')\n",
    "\n",
    "    #Which training data are images of faces\n",
    "    y.float()\n",
    "    face_indicator = (y == 1.0).float()\n",
    "\n",
    "    total_loss = torch.mean(classification_loss * face_indicator + vae_loss) #If face_indicator == 0, it will not consider the vae_loss since is not a face\n",
    "\n",
    "    return total_loss, classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed0bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_face_decoder_network(latent_dim=128, n_filters=12):\n",
    "    \"\"\"\n",
    "    Decoder network of a VAE model.\n",
    "    Args:\n",
    "        latent_dim (int): Dimension of the latent space.\n",
    "        n_filters (int): Number of filters in the convolutional layers.\n",
    "    Returns:\n",
    "        FaceDecoder (nn.Module): Decoder network model.\n",
    "    \"\"\"\n",
    "    class FaceDecoder(nn.Module):\n",
    "        def __init__(self, latent_dim, n_filters):\n",
    "            super(FaceDecoder, self).__init__()\n",
    "            self.latent_dim = latent_dim\n",
    "            self.n_filters = n_filters\n",
    "            self.linear = nn.Sequential(nn.Linear(latent_dim, 8 * self.n_filters * 4 * 4), nn.ReLU())\n",
    "        \n",
    "            self.deconv = nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=8 * self.n_filters,\n",
    "                    out_channels=4 * self.n_filters,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=4 * self.n_filters,\n",
    "                    out_channels=2 * self.n_filters,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=2 * self.n_filters,\n",
    "                    out_channels=self.n_filters,\n",
    "                    kernel_size=5,\n",
    "                    stride=2,\n",
    "                    padding=2,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=self.n_filters,\n",
    "                    out_channels=3,\n",
    "                    kernel_size=5,\n",
    "                    stride=2,\n",
    "                    padding=2,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "            )\n",
    "        def forward(self, z):\n",
    "            x = self.linear(z)\n",
    "            x = x.view(-1, 8*self.n_filters, 4, 4)\n",
    "            x = self.deconv(x)\n",
    "            return x\n",
    "    return FaceDecoder(latent_dim, n_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DB_VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Debiased Variational Autoencoder Model.\n",
    "    Args:\n",
    "        latent_dim (int): Dimension of the latent space.\n",
    "    Returns:\n",
    "        DB_VAE (nn.Module): Debiased VAE model.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(DB_VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = make_standard_classifier(n_outputs=2*latent_dim+1)\n",
    "        self.decoder = make_face_decoder_network(latent_dim=latent_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        encoder_out = self.encoder(x)\n",
    "\n",
    "        y_logit = encoder_out[:, 0].unsqueeze(-1)\n",
    "        z_mu = encoder_out[:, 1 : self.latent_dim + 1]\n",
    "        z_logsigma = encoder_out[:, self.latent_dim + 1 :]\n",
    "        return y_logit, z_mu, z_logsigma\n",
    "    \n",
    "    def reparameterize(self, z_mu, z_logsigma):\n",
    "        return sampling_reparameterization(z_mu, z_logsigma)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_logit, z_mu, z_logsigma = self.encode(x)\n",
    "        z = self.reparameterize(z_mu, z_logsigma)\n",
    "        recon = self.decode(z)\n",
    "        return y_logit, z_mu, z_logsigma, recon\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_logit, _, _ = self.encode(x)\n",
    "        return y_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_mu(images, dbvae):\n",
    "    \"\"\"\n",
    "    Get the latent mean vectors for a set of images using the DB-VAE model.\n",
    "    Args:\n",
    "        images (np.ndarray): Input images.\n",
    "        dbvae (DB_VAE): Debiased VAE model.\n",
    "    Returns:\n",
    "        mu (np.ndarray): Latent mean vectors for the input images.\n",
    "    \"\"\"\n",
    "    dbvae.eval()\n",
    "    all_z_mean = []\n",
    "    images_t = torch.from_numpy(images).float()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for start in range(0, len(images_t), params[\"batch_size\"]):\n",
    "            end = start + params[\"batch_size\"]\n",
    "            batch = images_t[start:end].to(device).permute(0, 3, 1, 2)\n",
    "            _, z_mean, _, _ = dbvae(batch)\n",
    "            all_z_mean.append(z_mean.detach().cpu())\n",
    "    \n",
    "    print(\"Number of batches:\", len(all_z_mean))\n",
    "    z_mean_full = torch.cat(all_z_mean, dim=0)\n",
    "    mu = z_mean_full.numpy()\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_sample_prob(images, dbvae, bins=10, smoothing_fac=0.001):\n",
    "    \"\"\"\n",
    "    Calculates the probability chance of images under represented in the latent variables\n",
    "    distribution, favoring images who are rarer in the distribution.\n",
    "    Args:\n",
    "        images (np.ndarray): Input images sample.\n",
    "        dbvae (DB_VAE): Debiased VAE model.\n",
    "        batch_size (int): Batch size for processing images.\n",
    "        bins (int): Number of histogram intervals used to estimate the latent variable distribution along each dimension.\n",
    "        smoothing_fac (float): Small float value to avoid zeros\n",
    "    Returns:\n",
    "        training_sample_p (np.ndarray): Sample probabilities where rare samples in the distribution have a high probability \n",
    "    \"\"\"\n",
    "    print(\"Recomputing the sample probabilities\")\n",
    "\n",
    "    mu = get_latent_mu(images, dbvae)\n",
    "    training_sample_p = np.zeros(mu.shape[0], dtype=np.float64)\n",
    "\n",
    "    for i in range(dbvae.latent_dim):\n",
    "        latent_distribution = mu[:, i]\n",
    "\n",
    "        hist_density, bin_edges = np.histogram(latent_distribution, density=True, bins=bins)\n",
    "\n",
    "        bin_edges[0] = -float(\"inf\")\n",
    "        bin_edges[-1] = float(\"inf\")\n",
    "\n",
    "        bin_idx = np.digitize(latent_distribution, bin_edges)\n",
    "\n",
    "        hist_smoothed_density = hist_density + smoothing_fac\n",
    "        hist_smoothed_density /= np.sum(hist_smoothed_density)\n",
    "\n",
    "        p = 1.0 / (hist_smoothed_density[bin_idx -1])\n",
    "\n",
    "        p /= np.sum(p)\n",
    "\n",
    "        training_sample_p = np.maximum(training_sample_p, p)\n",
    "    \n",
    "    training_sample_p /= np.sum(training_sample_p)\n",
    "    return training_sample_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f14a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    batch_size=32,\n",
    "    learning_rate=5e-4,\n",
    "    latent_dim=144,\n",
    "    num_epochs=2,\n",
    ")\n",
    "\n",
    "dbvae = DB_VAE(params[\"latent_dim\"]).to(device)\n",
    "optimizer = optim.Adam(dbvae.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "def debiasing_train_step(x, y):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_logit, z_mean, z_logsigma, x_recon = dbvae(x)\n",
    "\n",
    "    loss, _ = debiasing_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "all_faces = loader.get_all_train_faces()\n",
    "\n",
    "step=0\n",
    "for i in range(params[\"num_epochs\"]):\n",
    "    print(\"Starting epoch {}/{}\".format(i+1, params[\"num_epochs\"]))\n",
    "    p_faces = get_training_sample_prob(all_faces, dbvae)\n",
    "\n",
    "    for j in tqdm(range(len(loader)//params[\"batch_size\"])):\n",
    "        (x, y) = loader.get_batch(params[\"batch_size\"], p_pos=p_faces)\n",
    "        x = torch.from_numpy(x).float().to(device)\n",
    "        y = torch.from_numpy(y).float().to(device)\n",
    "\n",
    "        loss = debiasing_train_step(x, y)\n",
    "        loss_value = loss.detach().cpu().numpy()\n",
    "        \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbvae.to(device)\n",
    "dbvae_logits_list = []\n",
    "with torch.inference_mode():\n",
    "    for face in test_faces:\n",
    "        face = torch.from_numpy(np.array(face, dtype=np.float32)).to(device)\n",
    "        logits = dbvae.predict(face)\n",
    "        dbvae_logits_list.append(logits.detach().cpu().numpy())\n",
    "\n",
    "dbvae_logits_array = np.concatenate(dbvae_logits_list, axis=0)\n",
    "dbvae_logits_tensor = torch.from_numpy(dbvae_logits_array)\n",
    "dbvae_probs_tensor = torch.sigmoid(dbvae_logits_tensor)\n",
    "dbvae_probs_array = dbvae_probs_tensor.squeeze(dim=-1).numpy()\n",
    "\n",
    "xx = np.arange(len(keys))\n",
    "std_probs_mean = std_classfier_probs.mean(axis=1)\n",
    "dbvae_probs_mean = dbvae_probs_array.reshape(len(keys), -1).mean(axis=1)\n",
    "\n",
    "plt.bar(xx, std_probs_mean, width=0.2, label=\"Standard CNN\")\n",
    "plt.bar(xx + 0.2, dbvae_probs_mean, width=0.2, label=\"DB-VAE\")\n",
    "\n",
    "plt.xticks(xx, keys)\n",
    "plt.title(\"Network predictions on test dataset\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
